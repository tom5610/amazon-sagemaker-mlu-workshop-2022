{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Marketing with Amazon SageMaker Autopilot\n",
    "\n",
    "This notebook works well with the `Python 3 (Data Science)` kernel on SageMaker Studio.\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "1. [Downloading the dataset](#Downloading)\n",
    "1. [Upload the dataset to Amazon S3](#Uploading)\n",
    "1. [Setting up the SageMaker Autopilot Job](#Settingup)\n",
    "1. [Launching the SageMaker Autopilot Job](#Launching)\n",
    "1. [Tracking Sagemaker Autopilot Job Progress](#Tracking)\n",
    "1. [Results](#Results)\n",
    "1. [Cleanup](#Cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "[Amazon SageMaker Autopilot](https://aws.amazon.com/sagemaker/autopilot/) is an automated machine learning (commonly referred to as AutoML) solution for tabular datasets. You can use SageMaker Autopilot in different ways:\n",
    "\n",
    "- On autopilot (hence the name) or with human guidance\n",
    "- Without code (through the SageMaker Studio UI), or using the AWS SDKs.\n",
    "\n",
    "This notebook will explain the ML problem and dataset, then use AutoPilot service in SageMaker Studio UI without codeing to auto-train & deploy a model. Then, will use the AWS SDKs to simply create and deploy a machine learning model.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "A typical introductory task in machine learning (the \"Hello World\" equivalent) is one that uses a dataset to predict whether a customer will enroll for a term deposit at a bank, after one or more phone calls. For more information about the task and the dataset used, see [Bank Marketing Data Set](https://archive.ics.uci.edu/ml/datasets/bank+marketing).\n",
    "\n",
    "Direct marketing, through mail, email, phone, etc., is a common tactic to acquire customers.  Because resources and a customer's attention are limited, the goal is to only target the subset of prospects who are likely to engage with a specific offer.  Predicting those potential customers based on readily available information like demographics, past interactions, and environmental factors is a common machine learning problem. You can imagine that this task would readily translate to marketing lead prioritization in your own organization.\n",
    "\n",
    "This notebook demonstrates how you can use Autopilot on this dataset to get the most accurate ML pipeline through exploring a number of potential options, or \"candidates\". Each candidate generated by Autopilot consists of two steps. The first step performs automated feature engineering on the dataset and the second step trains and tunes an algorithm to produce a model. When you deploy this model, it follows similar steps. Feature engineering followed by inference, to decide whether the lead is worth pursuing or not. The notebook contains instructions on how to train the model as well as to deploy the model to perform batch predictions on a set of leads. Where it is possible, use the Amazon SageMaker Python SDK, a high level SDK, to simplify the way you interact with Amazon SageMaker.\n",
    "\n",
    "Other examples demonstrate how to customize models in various ways. For instance, models deployed to devices typically have memory constraints that need to be satisfied as well as accuracy. Other use cases have real-time deployment requirements and latency constraints. For now, keep it simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "In this first cell, we'll:\n",
    "\n",
    "- **configure** the S3 bucket and folder where data should be stored (to keep our environment tidy)\n",
    "- **connect** to SageMaker with the open-source [Sagemaker Python SDK](https://sagemaker.readthedocs.io/en/stable/), `sagemaker`\n",
    "\n",
    "Note that while [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) is the general-purpose AWS SDK for Python, `sagemaker` provides some powerful, higher-level interfaces designed specifically for ML workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "prefix = \"mlu-workshop/autopilot-dm\"\n",
    "\n",
    "sm = session.sagemaker_client\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the dataset<a name=\"Downloading\"></a>\n",
    "\n",
    "In this example we'll use the **direct marketing dataset** as per:\n",
    "\n",
    "> *\\[Moro et al., 2014\\] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014*\n",
    "\n",
    "Here, we'll download [the dataset](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip) from the SageMaker sample data S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-09 05:45:08--  https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip\n",
      "Resolving sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com (sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com)... 52.218.182.226\n",
      "Connecting to sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com (sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com)|52.218.182.226|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 432828 (423K) [application/zip]\n",
      "Saving to: ‘data/bank-additional.zip’\n",
      "\n",
      "bank-additional.zip 100%[===================>] 422.68K   764KB/s    in 0.6s    \n",
      "\n",
      "2022-02-09 05:45:09 (764 KB/s) - ‘data/bank-additional.zip’ saved [432828/432828]\n",
      "\n",
      "Unzipping...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!wget -P data/ -N https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip\n",
    "\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"data/bank-additional.zip\", \"r\") as zip_ref:\n",
    "    print(\"Unzipping...\")\n",
    "    zip_ref.extractall(\"data\")\n",
    "print(\"Done\")\n",
    "\n",
    "local_data_path = \"./data/bank-additional/bank-additional-full.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the dataset to Amazon S3<a name=\"Uploading\"></a>\n",
    "\n",
    "Before you run Autopilot on the dataset, first perform a check of the dataset to make sure that it has no obvious errors. The Autopilot process can take long time, and it's generally a good practice to inspect the dataset before you start a job. This particular dataset is small, so you can inspect it in the notebook instance itself. If you have a larger dataset that will not fit in a notebook instance memory, inspect the dataset offline using a big data analytics tool like Apache Spark. [Deequ](https://github.com/awslabs/deequ) is a library built on top of Apache Spark that can be helpful for performing checks on large datasets. Autopilot is capable of handling datasets up to 5 GB.\n",
    "\n",
    "Read the data into a Pandas data frame and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41183</th>\n",
       "      <td>73</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>334</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41184</th>\n",
       "      <td>46</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>383</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41185</th>\n",
       "      <td>56</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>189</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41186</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>442</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41187</th>\n",
       "      <td>74</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>239</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41188 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age          job  marital            education  default housing loan  \\\n",
       "0       56    housemaid  married             basic.4y       no      no   no   \n",
       "1       57     services  married          high.school  unknown      no   no   \n",
       "2       37     services  married          high.school       no     yes   no   \n",
       "3       40       admin.  married             basic.6y       no      no   no   \n",
       "4       56     services  married          high.school       no      no  yes   \n",
       "...    ...          ...      ...                  ...      ...     ...  ...   \n",
       "41183   73      retired  married  professional.course       no     yes   no   \n",
       "41184   46  blue-collar  married  professional.course       no      no   no   \n",
       "41185   56      retired  married    university.degree       no     yes   no   \n",
       "41186   44   technician  married  professional.course       no      no   no   \n",
       "41187   74      retired  married  professional.course       no     yes   no   \n",
       "\n",
       "         contact month day_of_week  duration  campaign  pdays  previous  \\\n",
       "0      telephone   may         mon       261         1    999         0   \n",
       "1      telephone   may         mon       149         1    999         0   \n",
       "2      telephone   may         mon       226         1    999         0   \n",
       "3      telephone   may         mon       151         1    999         0   \n",
       "4      telephone   may         mon       307         1    999         0   \n",
       "...          ...   ...         ...       ...       ...    ...       ...   \n",
       "41183   cellular   nov         fri       334         1    999         0   \n",
       "41184   cellular   nov         fri       383         1    999         0   \n",
       "41185   cellular   nov         fri       189         2    999         0   \n",
       "41186   cellular   nov         fri       442         1    999         0   \n",
       "41187   cellular   nov         fri       239         3    999         1   \n",
       "\n",
       "          poutcome  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  \\\n",
       "0      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "1      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "2      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "3      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "4      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "...            ...           ...             ...            ...        ...   \n",
       "41183  nonexistent          -1.1          94.767          -50.8      1.028   \n",
       "41184  nonexistent          -1.1          94.767          -50.8      1.028   \n",
       "41185  nonexistent          -1.1          94.767          -50.8      1.028   \n",
       "41186  nonexistent          -1.1          94.767          -50.8      1.028   \n",
       "41187      failure          -1.1          94.767          -50.8      1.028   \n",
       "\n",
       "       nr.employed    y  \n",
       "0           5191.0   no  \n",
       "1           5191.0   no  \n",
       "2           5191.0   no  \n",
       "3           5191.0   no  \n",
       "4           5191.0   no  \n",
       "...            ...  ...  \n",
       "41183       4963.6  yes  \n",
       "41184       4963.6   no  \n",
       "41185       4963.6   no  \n",
       "41186       4963.6  yes  \n",
       "41187       4963.6   no  \n",
       "\n",
       "[41188 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(local_data_path)\n",
    "with pd.option_context(\"display.max_columns\", 500):\n",
    "    # Make sure we can see all of the columns\n",
    "    display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 20 features to help predict the target column 'y'.\n",
    "\n",
    "Amazon SageMaker Autopilot takes care of preprocessing your data for you. You do not need to perform conventional data preprocssing techniques such as handling missing values, converting categorical features to numeric features, scaling data, and handling more complicated data types.\n",
    "\n",
    "Moreover, splitting the dataset into training and validation splits is not necessary. Autopilot takes care of this for you. You may, however, want to split out a test set. That's next, although you use it for batch inference at the end instead of testing the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reserve some data for calling batch inference on the model\n",
    "\n",
    "Divide the data into training and testing splits. The training split is used by SageMaker Autopilot. The testing split is reserved to perform inference using the suggested model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.sample(frac=0.8, random_state=200)\n",
    "\n",
    "test_data = data.drop(train_data.index)\n",
    "\n",
    "test_data_no_target = test_data.drop(columns=[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the dataset to Amazon S3\n",
    "Copy the file to Amazon Simple Storage Service (Amazon S3) in a .csv format for Amazon SageMaker training to use.\n",
    "\n",
    "**Please note down the S3 object URI of variable `train_data_s3_path`, which will be used in doing AutoPilot job in SageMaker Studio UI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data uploaded to: s3://sagemaker-ap-southeast-2-452533547478/mlu-workshop/autopilot-dm/train/train_data.csv\n",
      "Test data uploaded to: s3://sagemaker-ap-southeast-2-452533547478/mlu-workshop/autopilot-dm/test/test_data.csv\n"
     ]
    }
   ],
   "source": [
    "train_file = \"data/train_data.csv\"\n",
    "train_data.to_csv(train_file, index=False, header=True)\n",
    "train_data_s3_path = session.upload_data(path=train_file, key_prefix=prefix + \"/train\")\n",
    "print(\"Train data uploaded to: \" + train_data_s3_path)\n",
    "\n",
    "test_file = \"data/test_data.csv\"\n",
    "test_data_no_target.to_csv(test_file, index=False, header=False)\n",
    "test_data_s3_path = session.upload_data(path=test_file, key_prefix=prefix + \"/test\")\n",
    "print(\"Test data uploaded to: \" + test_data_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1 - Running AutoPilot in SageMaker Studio UI\n",
    "\n",
    "SageMaker Studio UI provides an easy way to trigger AutoPilot job so that you can start your model experiment quickly. In this section, we will work through the steps to create AutoPilot job and do some predictions on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open Amazon SageMaker Studio in console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Under 'Launcher' tab, choose the **New autopilot experiment** option from the **Build model automatically** box. (If you don't see 'Launcher' tab, you can open one under Menu 'File' -> 'New Launcher'\n",
    " <br />\n",
    " \n",
    "![New autoPilot experiment](./image/ap_new_autopilot_experiment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Enter information for the AutoPilot Job\n",
    "\n",
    " * **Experiment name** - an unique name to your account in the current AWS Region and container a maximum of 63 alphanumeric characters. Can include hyphens(-). e.g. 'direct-marketing-autopilot-job'\n",
    " \n",
    " <br />\n",
    " \n",
    " ![experiment name](./image/ap_experiment_name.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * **Connect your data** - Provide the training data S3 URI.\n",
    " \n",
    " <br />\n",
    " \n",
    " ![experiment name](./image/ap_enter_s3bucket_location.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * **Is your S3 input a manifest file?** - choose 'off' for the lab given we don't have a manifest file include meta data for our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **target** - the target value or label in the training dataset.\n",
    "  \n",
    "  ![manifest file](./image/ap_target.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Output data location** - the name of the S3 bucket and directory where you want to store the output data.\n",
    " \n",
    "  ![manifest file](./image/ap_output_data_location.png)\n",
    "  \n",
    "  > **_NOTE_**: You may select a S3 bucket (which is under the AWS Region) and related directory, or provide the S3 folder URI. In our exercise, please use a directory under SageMaker Default S3 bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Select the machine learning problem type** - Autopilot can automatically select the machine learning problem type and you can specify manually. In our exercise, please choose `Binary classification` in dropdox box.\n",
    "  \n",
    "  ![manifest file](./image/ap_ml_problem_type.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Do you want to run complete experiment** - You can specify how to run the experiment. \n",
    "  \n",
    "  ![manifest file](./image/ap_complete_experiment.png)\n",
    "  \n",
    "   * If you choose **Yes**, Autopilot runs experiments with model training, generates related trials and you will be able to deploy the best model to SageMaker Endpoint service for realtime inference. \n",
    "   * If you choose **No**, instead of running the entire experiment, AutoPilot stops running after generating the notebooks for dataset analysis & candidates definitions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Auto deploy** - Autopilot can automatically deploy the best model from an Autopilot experiment to an endpoint (for realtime inference), accept the default Auto deploy value **On** when creating the experiment. Also, please provide the endpoint name. In our exercise, please input `dm-autopilot-experiment`.\n",
    "  \n",
    "  ![manifest file](./image/ap_auto_deploy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **ADVANCED SETTINGS** - The settings allows you to specify how the experiment should be run. Especially, we want to set the max candidate to be experimented as `10` and accept default values for others.\n",
    "  \n",
    "  ![manifest file](./image/ap_advanced_settings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Auto deploy the best model confirmation?** - If you choose **On** under 'Auto Deploy', Autopilot will prompt a confirmation dialog to remind you that it will generate cost while deploying the model to SageMaker endpoint. In our exercise, please click `Confirm` button.\n",
    "  \n",
    "  ![manifest file](./image/ap_prompt_best_model_deployment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **AutoPilot experiment in progress** - Once the Autopilot experiment is kicked off, you will be able to view the progress of the experiment.\n",
    "  \n",
    "  ![manifest file](./image/ap_auto_pilot_job_in_progress.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options 2 - Running AutoPilot with SageMaker Python SDK\n",
    "\n",
    "## Setting up the SageMaker Autopilot Job<a name=\"Settingup\"></a>\n",
    "\n",
    "After uploading the dataset to Amazon S3, you can invoke Autopilot to find the best ML pipeline to train a model on this dataset. \n",
    "\n",
    "The required inputs for invoking a Autopilot job are:\n",
    "* Amazon S3 location for input dataset and for all output artifacts\n",
    "* Name of the column of the dataset you want to predict (`y` in this case) \n",
    "* An IAM role\n",
    "\n",
    "Currently Autopilot supports only tabular datasets in CSV format. Either all files should have a header row, or the first file of the dataset, when sorted in alphabetical/lexical order, is expected to have a header row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "from sagemaker.automl import automl\n",
    "\n",
    "timestamp_suffix = f\"{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "auto_ml_job_name = f\"automl-banking-{timestamp_suffix}\"\n",
    "print(f\"AutoMLJobName: {auto_ml_job_name}\")\n",
    "\n",
    "automl_job = automl.AutoML(\n",
    "    role=role,\n",
    "    target_attribute_name=\"y\",\n",
    "    output_path=f\"s3://{bucket}/{prefix}/automl-output\",\n",
    "    problem_type=\"BinaryClassification\",\n",
    "    max_candidates=10,  # (We've set this low to prioritize demo speed over accuracy)\n",
    "    job_objective={\"MetricName\": \"F1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the type of problem you want to solve with your dataset (`Regression, MulticlassClassification, BinaryClassification`) is **optional**. In case you are not sure, SageMaker Autopilot will infer the problem type based on statistics of the target column (the column you want to predict). \n",
    "\n",
    "You have the option to limit the running time of a SageMaker Autopilot job by providing either the maximum number of pipeline evaluations or candidates (one pipeline evaluation is called a `Candidate` because it generates a candidate model) or providing the total time allocated for the overall Autopilot job. Under default settings, this job takes about four hours to run. This varies between runs because of the nature of the exploratory process Autopilot uses to find optimal training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching the SageMaker Autopilot Job<a name=\"Launching\"></a>\n",
    "\n",
    "You can now launch the Autopilot job by calling the `fit()` method as described in the [SageMaker Python SDK AutoML doc](https://sagemaker.readthedocs.io/en/stable/api/training/automl.html#sagemaker.automl.automl.AutoML.fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_job.fit(inputs=train_data_s3_path, wait=False, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking SageMaker Autopilot job progress<a name=\"Tracking\"></a>\n",
    "SageMaker Autopilot job consists of the following high-level steps : \n",
    "* Analyzing Data, where the dataset is analyzed and Autopilot comes up with a list of ML pipelines that should be tried out on the dataset. The dataset is also split into train and validation sets.\n",
    "* Feature Engineering, where Autopilot performs feature transformation on individual features of the dataset as well as at an aggregate level.\n",
    "* Model Tuning, where the top performing pipeline is selected along with the optimal hyperparameters for the training algorithm (the last stage of the pipeline). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"JobStatus - Secondary Status\\n----------------------------\")\n",
    "\n",
    "while True:\n",
    "    sleep(60)\n",
    "    describe_response = automl_job.describe_auto_ml_job()\n",
    "    print(\"{AutoMLJobStatus} - {AutoMLJobSecondaryStatus}\".format(**describe_response))\n",
    "    if describe_response[\"AutoMLJobStatus\"] in (\"Failed\", \"Completed\", \"Stopped\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Results\n",
    "\n",
    "The Autopilot job is completed, and we now have a set of models with their associated performance metric.\n",
    "Let's consider the top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_list = automl_job.list_candidates(\n",
    "    max_results=10, sort_by=\"FinalObjectiveMetricValue\"\n",
    ")\n",
    "\n",
    "models = pd.json_normalize(candidates_list)[\n",
    "    [\n",
    "        \"CandidateName\",\n",
    "        \"FinalAutoMLJobObjectiveMetric.Value\",\n",
    "        \"FinalAutoMLJobObjectiveMetric.MetricName\",\n",
    "    ]\n",
    "].rename(\n",
    "    columns={\n",
    "        \"FinalAutoMLJobObjectiveMetric.Value\": \"metric_value\",\n",
    "        \"FinalAutoMLJobObjectiveMetric.MetricName\": \"metric_name\",\n",
    "        \"CandidateName\": \"candidate_name\",\n",
    "    }\n",
    ")\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_job.best_candidate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Perform batch inference using the best candidate\n",
    "\n",
    "Now that you have successfully completed the SageMaker Autopilot job on the dataset, create a model from any of the candidates by using [Inference Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html). \n",
    "\n",
    "For classification problem types, the inference containers generated by SageMaker Autopilot allow you to select the response content for predictions. Valid inference response content are defined below for binary classification and multiclass classification problem types.\n",
    "\n",
    "- `predicted_label` - predicted class\n",
    "- `probability` - In binary classification, the probability that the result is predicted as the second or True class in the target column. In multiclass classification, the probability of the winning class.\n",
    "- `labels` - list of all possible classes\n",
    "- `probabilities` - list of all probabilities for all classes (order corresponds with `labels`)\n",
    "\n",
    "By default the inference contianers are configured to generate the `predicted_label` only.\n",
    "\n",
    "In this binary classification example we'll request both `predicted_label` and `probability` - demonstrating how this additional \"confidence\" output from the model can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"automl-banking-model-\" + timestamp_suffix\n",
    "inference_response_keys = [\"predicted_label\", \"probability\"]\n",
    "model = automl_job.create_model(\n",
    "    name=model_name,\n",
    "    candidate=automl_job.best_candidate(),\n",
    "    inference_response_keys=inference_response_keys,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use batch inference by using Amazon SageMaker batch transform. The same model can also be deployed to perform online inference using Amazon SageMaker hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"s3://{bucket}/{prefix}/inference-results/\"\n",
    "\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    assemble_with=\"Line\",\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start the transform job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_job = transformer.transform(\n",
    "    data=test_data_s3_path,\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\",\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch the transform job for completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"JobStatus\\n----------\")\n",
    "\n",
    "while True:\n",
    "    sleep(30)\n",
    "    describe_response = sm.describe_transform_job(\n",
    "        TransformJobName=transformer._current_job_name\n",
    "    )\n",
    "    job_run_status = describe_response[\"TransformJobStatus\"]\n",
    "    print(job_run_status)\n",
    "    if job_run_status in (\"Failed\", \"Completed\", \"Stopped\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's view the results of the transform job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_preds = pd.read_csv(\n",
    "    transformer.output_path + \"test_data.csv.out\",\n",
    "    header=None,\n",
    "    names=inference_response_keys,\n",
    ")\n",
    "\n",
    "test_data_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the result of the transform job to evaluate additional metrics on the test dataset, using the [scikit-learn](https://scikit-learn.org/stable/index.html) library.\n",
    "\n",
    "Common metrics for classification problems are [AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) and [AP](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "labels = test_data[\"y\"]\n",
    "AUC = roc_auc_score(labels == \"yes\", test_data_preds.probability)\n",
    "AP = average_precision_score(labels, test_data_preds.probability, pos_label=\"yes\")\n",
    "\n",
    "print(f\"AUC: {AUC:.3f}\\nAP {AP:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate a classification report and a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels == \"yes\", test_data_preds.predicted_label == \"yes\"))\n",
    "cm = confusion_matrix(labels, test_data_preds.predicted_label, labels=[\"yes\", \"no\"])\n",
    "ConfusionMatrixDisplay(cm, display_labels=[\"yes\", \"no\"]).plot(\n",
    "    include_values=[\"yes\", \"no\"], cmap=plt.cm.Blues, values_format=\"d\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And present the model performance using [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) and Precision-Recall curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, [ax0, ax1] = plt.subplots(1, 2, figsize=(16, 9))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(labels == \"yes\", test_data_preds.probability)\n",
    "ax0.step(fpr, tpr, where=\"post\")\n",
    "ax0.fill_between(fpr, tpr, step=\"post\", alpha=0.2, color=\"b\")\n",
    "ax0.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "ax0.set_xlabel(\"False Positive Rate\")\n",
    "ax0.set_ylabel(\"True Positive Rate\")\n",
    "ax0.set_title(\"ROC Curve\")\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(\n",
    "    labels == \"yes\", test_data_preds.probability\n",
    ")\n",
    "ax1.step(recall, precision, where=\"post\")\n",
    "ax1.fill_between(recall, precision, step=\"post\", alpha=0.2, color=\"b\")\n",
    "ax1.set_xlabel(\"Recall\")\n",
    "ax1.set_ylabel(\"Precision\")\n",
    "ax1.set_title(\"Precision-Recall Curve\")\n",
    "\n",
    "for ax in [ax0, ax1]:\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_aspect(1)\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration and Modelling Notebooks\n",
    "\n",
    "As well as the results and candidate models themselves, Autopilot generates other artifacts including:\n",
    "\n",
    "- A **Data Exploration Notebook**: produced during the analysis phase of the job, that helps you identify problems in your dataset.\n",
    "- A **Candidate Definitions Notebook**: interactively stepping through the steps taken by Autopilot to define and train candidates, and select the best one.\n",
    "- **Supporting Python code**: Including the actual code used for the different pre-processing steps.\n",
    "\n",
    "To get a good overview of the available assets, we'll download not just the notebooks but the whole output folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_job_desc = automl_job.describe_auto_ml_job()\n",
    "\n",
    "automl_output_s3uri = automl_job_desc[\"OutputDataConfig\"][\"S3OutputPath\"]\n",
    "print(f\"Autopilot output:\\n{automl_output_s3uri}\")\n",
    "\n",
    "print(f\"Downloading to autopilot_output/...\")\n",
    "sagemaker.s3.S3Downloader.download(automl_output_s3uri, \"autopilot_output/\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this download we can view not just the notebooks, but also other assets like the generated Python code they link to, and pre-processed datasets. Explore the notebooks linked below, but also check out the other contents in the `autopilot_output` folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "candidate_notebook_s3uri = automl_job_desc[\"AutoMLJobArtifacts\"][\n",
    "    \"CandidateDefinitionNotebookLocation\"\n",
    "]\n",
    "candidate_notebook_path = \"autopilot_output\" + candidate_notebook_s3uri[len(automl_output_s3uri):]\n",
    "\n",
    "print(f\"Candidate definition notebook:\\n{candidate_notebook_s3uri}\")\n",
    "print(f\"\\nDownloaded at:\")\n",
    "display(Markdown(f\"[{candidate_notebook_path}]({candidate_notebook_path})\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataexp_notebook_s3uri = automl_job_desc[\"AutoMLJobArtifacts\"][\n",
    "    \"DataExplorationNotebookLocation\"\n",
    "]\n",
    "dataexp_notebook_path = \"autopilot_output\" + dataexp_notebook_s3uri[len(automl_output_s3uri):]\n",
    "\n",
    "print(f\"Data exploration notebook:\\n{dataexp_notebook_s3uri}\")\n",
    "print(f\"\\nDownloaded at:\")\n",
    "display(Markdown(f\"[{dataexp_notebook_path}]({dataexp_notebook_path})\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Explainability Artifacts\n",
    "SageMaker AutoPilot uses SageMaker Clarify to generate an explainability report for the best candidate.\n",
    "\n",
    "These Clarify artifacts are also available from S3, and were already included in our download above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainability_s3uri = automl_job.best_candidate()[\"CandidateProperties\"][\n",
    "    \"CandidateArtifactLocations\"\n",
    "][\"Explainability\"]\n",
    "explainability_path = \"autopilot_output\" + explainability_s3uri[len(automl_output_s3uri):]\n",
    "\n",
    "print(f\"Explainability artifacts:\\n{explainability_s3uri}\")\n",
    "print(f\"\\nDownloaded to folder:\")\n",
    "print(f\"{explainability_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "The Autopilot job creates many underlying artifacts such as dataset splits, preprocessing scripts, or preprocessed data, etc. This code, when un-commented, deletes them. This operation deletes all the generated models and the auto-generated notebooks as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "\n",
    "# s3 = boto3.resource('s3')\n",
    "# bucket = s3.Bucket(bucket)\n",
    "\n",
    "# job_outputs_prefix = '{}/output/{}'.format(prefix, auto_ml_job_name)\n",
    "# bucket.objects.filter(Prefix=job_outputs_prefix).delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
