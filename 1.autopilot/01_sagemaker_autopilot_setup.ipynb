{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01 Setup\n",
    "\n",
    "This notebook works well with the `Python 3 (Data Science)` kernel on SageMaker Studio.\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Contents\n",
    "\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "1. [Read CSV dataset](#Read-CSV-dataset)\n",
    "1. [Prepare and upload training data to Amazon S3](#Prepare-and-upload-training-data-to-Amazon-S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "> Please execute [Setup and Data Preparation](../0.setup/setup_and_data_prep.ipynb) notebook before running AutoPilot experiment jobs.\n",
    "\n",
    "In this notebook, we are going to split the downloaded dataset and upload them to S3 so that we are ready to kick off Autopilot experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore the shared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r bucket\n",
    "%store -r prefix\n",
    "%store -r data_folder\n",
    "%store -r data_file_path\n",
    "\n",
    "lab_ap_prefix = f\"{prefix}/autopilot\"\n",
    "lab_ap_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the dataset into a Pandas data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_file_path)\n",
    "with pd.option_context(\"display.max_columns\", 500):\n",
    "    # Make sure we can see all of the columns\n",
    "    display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 20 features to help predict the target column 'y'.\n",
    "\n",
    "Amazon SageMaker Autopilot takes care of preprocessing your data for you. You do not need to perform conventional data preprocssing techniques such as handling missing values, converting categorical features to numeric features, scaling data, and handling more complicated data types.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and upload training data to Amazon S3\n",
    "Moreover, splitting the dataset into training and validation splits is not necessary. Autopilot takes care of this for you. You may, however, want to split out a test set. That's next, although you use it for batch inference at the end instead of testing the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reserve some data for calling batch inference on the model \n",
    "\n",
    "Divide the data into training and testing splits. The training split is used by SageMaker Autopilot. The testing split is reserved to perform inference using the suggested model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.sample(frac=0.8, random_state=200)\n",
    "\n",
    "test_data = data.drop(train_data.index)\n",
    "\n",
    "test_data_no_target = test_data.drop(columns=[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Upload the prepared training dataset to Amazon S3 for the Autopilot experiment </b>\n",
    "\n",
    "Copy the file to Amazon Simple Storage Service (Amazon S3) in a .csv format for Amazon SageMaker training to use.\n",
    "\n",
    "**Please note down the S3 object URI of variable `train_data_s3_path`, which will be used in doing AutoPilot job in SageMaker Studio UI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = f\"{data_folder}/train_data.csv\"\n",
    "train_data.to_csv(train_file, index=False, header=True)\n",
    "train_data_s3_path = session.upload_data(path=train_file, key_prefix=lab_ap_prefix + \"/train\")\n",
    "print(\"Train data uploaded to: \" + train_data_s3_path)\n",
    "\n",
    "test_file = f\"{data_folder}/test_data_without_label.csv\"\n",
    "test_data_no_target.to_csv(test_file, index=False, header=False)\n",
    "test_data_without_label_s3_path = session.upload_data(path=test_file, key_prefix=lab_ap_prefix + \"/test\")\n",
    "print(\"Test data without label uploaded to: \" + test_data_without_label_s3_path)\n",
    "\n",
    "test_file_with_label = f\"{data_folder}/test_data_with_label.csv\"\n",
    "test_data.to_csv(test_file_with_label, index=False, header=True)\n",
    "# test_data_with_label_s3_path = session.upload_data(path=test_file_with_label, key_prefix=lab_ap_prefix + \"/test\")\n",
    "# print(\"Test data with label uploaded to: \" + test_data_with_label_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the shared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store lab_ap_prefix\n",
    "%store train_data_s3_path\n",
    "%store test_data_without_label_s3_path\n",
    "%store test_file_with_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next\n",
    "\n",
    "Since training data is now stored on S3 bucket, we are ready to kick off an Autopilot experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
